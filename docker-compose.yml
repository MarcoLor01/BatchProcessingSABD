services:

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_namenode_rpc-address=namenode:9000
    ports:
      - "9870:9870"    # Web UI HDFS
      - "9000:9000"    # HDFS default FS port
    volumes:
      - namenode_data:/hadoop/dfs/name
      - ./hadoop_conf:/hadoop/etc/hadoop
    networks:
      - data_network
    restart: always

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    depends_on:
      - namenode
    volumes:
      - datanode_data:/hadoop/dfs/data
      - ./hadoop_conf:/hadoop/etc/hadoop
    networks:
      - data_network
    restart: always

  nifi:
    image: apache/nifi:1.15.3  # Versione stabile e pi√π facile da configurare
    container_name: nifi
    ports:
      - "8080:8080"
      - "8443:8443"  # Aggiungi anche la porta HTTPS
    depends_on:
      - namenode
      - datanode
    environment:
      - NIFI_WEB_HTTP_PORT=8080  # Forza HTTP
      - NIFI_WEB_HTTP_HOST=0.0.0.0
      - NIFI_WEB_HTTPS_PORT=8443
      - NIFI_WEB_HTTPS_HOST=0.0.0.0
      - NIFI_SENSITIVE_PROPS_KEY=my-sensitive-key-12345

      - SINGLE_USER_CREDENTIALS_USERNAME=admin
      - SINGLE_USER_CREDENTIALS_PASSWORD=adminadmin12345
      - NIFI_SECURITY_USER_AUTHORIZER=single-user-authorizer
      - NIFI_SECURITY_USER_LOGIN_IDENTITY_PROVIDER=single-user-provider
    volumes:
      - nifi_conf:/opt/nifi/nifi-current/conf
      - nifi_state:/opt/nifi/nifi-current/state
      - nifi_content_repository:/opt/nifi/nifi-current/content_repository
      - nifi_database_repository:/opt/nifi/nifi-current/database_repository
      - nifi_flowfile_repository:/opt/nifi/nifi-current/flowfile_repository
      - nifi_provenance_repository:/opt/nifi/nifi-current/provenance_repository
      - ./hadoop_conf:/opt/hadoop/etc/hadoop
    networks:
      - data_network
    restart: always

  spark-master:
    container_name: da-spark-master
    build: .
    image: da-spark-image
    entrypoint: [ './entrypoint.sh', 'master' ]
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080" ]
      interval: 5s
      timeout: 3s
      retries: 3
    volumes:
      - ./scripts:/opt/spark/scripts
      - spark-logs:/opt/spark/spark-events
      - ./hadoop_conf:/opt/hadoop/etc/hadoop
    env_file:
      - .env.spark
    networks:
      - data_network
    ports:
      - '9090:8080'
      - '7077:7077'

  spark-worker:
    container_name: da-spark-worker
    image: da-spark-image
    entrypoint: [ './entrypoint.sh', 'worker' ]
    depends_on:
      - spark-master
    env_file:
      - .env.spark
    volumes:
      - ./scripts:/opt/spark/scripts
      - spark-logs:/opt/spark/spark-events
      - ./hadoop_conf:/opt/hadoop/etc/hadoop
    networks:
      - data_network

  spark-history-server:
    container_name: da-spark-history
    image: da-spark-image
    entrypoint: [ './entrypoint.sh', 'history' ]
    depends_on:
      - spark-master
    env_file:
      - .env.spark
    volumes:
      - spark-logs:/opt/spark/spark-events
    ports:
      - '18080:18080'

networks:
  data_network:
    driver: bridge

volumes:
  namenode_data:
  datanode_data:
  nifi_conf:
  nifi_state:
  nifi_content_repository:
  nifi_database_repository:
  nifi_flowfile_repository:
  nifi_provenance_repository:
  spark-logs: